{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22bd563a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import cv2\n",
    "import ipywidgets as widgets\n",
    "import asyncio\n",
    "import matplotlib.pyplot as plt\n",
    "import skimage\n",
    "import git\n",
    "import sys\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "import math\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import scipy\n",
    "import argparse\n",
    "from os import listdir\n",
    "from os.path import isfile, join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1f9b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#screen_h, screen_w = 1440, 2560\n",
    "#screen_h, screen_w = 1440//4, 2560//4\n",
    "screen_h, screen_w = 200, 200\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU is available\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU not available, using CPU instead\")\n",
    "\n",
    "\n",
    "def NormalizeEnergy(energy, width=3):\n",
    "    mean = torch.mean(energy.flatten())\n",
    "    std = torch.std(energy.flatten())\n",
    "    h_cutoff = mean + std*width\n",
    "    energy_normed = (torch.clip(energy, 0, h_cutoff))/(h_cutoff)\n",
    "    return energy_normed\n",
    "\n",
    "def NormalizeSine(arr):\n",
    "    return (arr + 1.0)/2.0\n",
    "    \n",
    "def Colorize(arr):\n",
    "    '''\n",
    "    Assumes a tensor of shape (h, w) and outputs \n",
    "    (h, w, (rgb))\n",
    "    '''\n",
    "    # First convert to CIELAB space\n",
    "    lab = np.stack([60*np.ones_like(arr), 128*np.cos(arr), 128*np.sin(arr)], axis=-1)\n",
    "    rgb = skimage.color.lab2rgb(lab)\n",
    "    return rgb\n",
    "    \n",
    "def lab2rgb(lab):\n",
    "    \"\"\"\n",
    "    Convert CIELAB color space to RGB color space.\n",
    "    \n",
    "    Args:\n",
    "        lab (torch.Tensor): CIELAB color tensor of shape (N, 3), where N is the number of colors.\n",
    "                            The L* component should be in the range [0, 100], and the a* and b*\n",
    "                            components should be in the range [-128, 127].\n",
    "    \n",
    "    Returns:\n",
    "        torch.Tensor: RGB color tensor of shape (N, 3), with values in the range [0, 1].\n",
    "    \"\"\"\n",
    "    # Convert L* from [0, 100] to [0, 1]\n",
    "    #L = lab[:, 0] / 100.0\n",
    "    L = 60*torch.zeros_like(lab)\n",
    "    \n",
    "    # Convert a* and b* from [-128, 127] to [-1, 1]\n",
    "    #a = lab[:, 1] / 128.0\n",
    "    #b = lab[:, 2] / 128.0\n",
    "    a = torch.cos(lab)\n",
    "    b = torch.sin(lab)\n",
    "    \n",
    "    # Compute intermediate values\n",
    "    var_Y = (L + 16) / 116\n",
    "    var_X = a / 500 + var_Y\n",
    "    var_Z = var_Y - b / 200\n",
    "\n",
    "    # Compute XYZ values\n",
    "    #X = 95.047 *  ((var_X ** 3)*(var_X > 0.008856) +  (var_X > 0.008856)*((var_X - 16/116) / 7.787))\n",
    "    #Y = 100.000 * ((var_Y ** 3)*(var_Y > 0.008856) + (var_Y > 0.008856)*((var_Y - 16/116) / 7.787))\n",
    "    #Z = 108.883 * ((var_Z ** 3)*(var_Z > 0.008856) + (var_Z > 0.008856)*((var_Z - 16/116) / 7.787))\n",
    "    X = 95.047 *  (var_X ** 3)\n",
    "    Y = 100.000 * (var_Y ** 3)\n",
    "    Z = 108.883 * (var_Z ** 3)\n",
    "    \n",
    "    # Convert XYZ to RGB\n",
    "    XYZ = torch.stack([X, Y, Z], dim=-1) / 100.0\n",
    "    RGB = torch.matmul(XYZ, torch.tensor([\n",
    "        [3.2406, -1.5372, -0.4986],\n",
    "        [-0.9689, 1.8758, 0.0415],\n",
    "        [0.0557, -0.2040, 1.0570]\n",
    "    ]).cuda())\n",
    "    \n",
    "    # Clip RGB values to [0, 1]\n",
    "    RGB = torch.clamp(RGB, 0.0, 1.0)\n",
    "    \n",
    "    return torch.permute(torch.squeeze(RGB), (2, 0, 1))[None, ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc3109f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RGB --> Init Phase, Coupling Constants foreach scale\n",
    "# (H, W, 3) --> (H, W, 1), (H, W, N_convs)\n",
    "\n",
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self, input_chans=3, num_scales=3):\n",
    "        super(AutoEncoder, self).__init__()\n",
    "        ic = input_chans\n",
    "        self.num_scales = num_scales\n",
    "        # Convolutions for common feature extractor\n",
    "        self.conv1 = nn.Conv2d(ic,  8, kernel_size=5, stride=1, padding='same')\n",
    "        self.conv2 = nn.Conv2d( 8, 16, kernel_size=5, stride=1, padding='same')\n",
    "        self.conv3 = nn.Conv2d(16,  8, kernel_size=5, stride=1, padding='same')\n",
    "        self.conv4 = nn.Conv2d( 8,  8, kernel_size=5, stride=1, padding='same')\n",
    "        self.conv5 = nn.Conv2d( 8,  8, kernel_size=5, stride=1, padding='same')\n",
    "        self.conv6 = nn.Conv2d( 8,  8, kernel_size=5, stride=1, padding='same')\n",
    "        self.conv_phases = nn.Conv2d( 8,  1, kernel_size=5, stride=1, padding='same')\n",
    "        self.conv_couple = nn.Conv2d( 8,  self.num_scales, kernel_size=5, stride=1, padding='same')\n",
    "        \n",
    "    def forward(self, x):\n",
    "        ## 1 - Extract features\n",
    "        # Convert image into amplitude, frequency, and phase shift for our CCs.\n",
    "        x = self.conv1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.conv3(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.conv4(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.conv5(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.conv6(x)\n",
    "        x = torch.relu(x)\n",
    "        z_phases = self.conv_phases(x)\n",
    "        z_couple = self.conv_couple(x)\n",
    "        \n",
    "        return z_phases, z_couple\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62a7c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate convolution dict\n",
    "coupling_convs = {}\n",
    "num_scales = 5\n",
    "size = 3\n",
    "grow = 3\n",
    "for i in range(num_scales):\n",
    "    conv = torch.nn.Conv2d(1, 1, size, bias=False, padding='same', padding_mode='circular', device=device)\n",
    "    if(i == 0):\n",
    "        k = torch.ones((size, size)).to(device)\n",
    "        k = k /(torch.sum(k) - 1.0)\n",
    "    else:\n",
    "        k = torch.zeros((last_size, last_size)).to(device)\n",
    "        k = torch.nn.functional.pad(k, (grow, grow, grow, grow), value=1.0)\n",
    "        k = k /(torch.sum(k))\n",
    "    \n",
    "    \n",
    "    k[k.shape[0]//2, k.shape[0]//2] = -1\n",
    "    print(size, k.shape, conv.weight.shape, k.sum())\n",
    "    conv.weight = torch.nn.Parameter(k[None, None, ...])\n",
    "    coupling_convs[k.shape[0]] = conv\n",
    "    last_size = size\n",
    "    size += grow*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c13b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_sim(phases, coupling_constants, coupling_convs, frequency = 0.1, num_steps = 500, show_sim=False):\n",
    "    #img_mean = torch.zeros_like(phases).cuda()\n",
    "    #img_mean = torch.zeros_like(lab2rgb(phases).cuda())\n",
    "    img_mean = None\n",
    "    for t in range(num_steps):\n",
    "        # Calculate the oscillator's positions\n",
    "        img_out = torch.sin(frequency*t + phases)\n",
    "        #img_mean += img_out\n",
    "        # Calculate the errors\n",
    "        error_sum = torch.zeros_like(img_out)\n",
    "        for k_idx, k_key in enumerate(coupling_convs):\n",
    "            error = torch.sin(coupling_convs[k_key](phases))\n",
    "            error_sum -= error*coupling_constants[0, k_idx]\n",
    "            #print(t, torch.abs(error_sum).mean())\n",
    "        phases += error_sum\n",
    "        \n",
    "        if(img_mean is None):\n",
    "            img_mean = lab2rgb(phases[0,0,...])\n",
    "        else:\n",
    "            img_mean += lab2rgb(phases[0,0,...])\n",
    "        #print(img_out[0,0,...].shape, Colorize(img_out[0,0,...].cpu().detach().numpy()).shape)\n",
    "        if(((t % 10) == 0) and show_sim):\n",
    "            img_out_waves = Colorize(img_out[0,0,...].cpu().detach().numpy())\n",
    "            img_out_phases = Colorize(phases[0,0,...].cpu().detach().numpy())\n",
    "            cv2.imshow('Waves', img_out_waves)\n",
    "            cv2.imshow('Phases', img_out_phases)\n",
    "            cv2.waitKey(1)\n",
    "    return img_mean / num_steps\n",
    "\n",
    "\n",
    "#run_sim(torch.rand((1, 1, screen_h, screen_w)).cuda(), torch.ones((1, 1, screen_h, screen_w, num_scales)).cuda(), coupling_convs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2987a828",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate image\n",
    "# Put through encoder\n",
    "# Put through simulator --> generating an average of output\n",
    "# Generate loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dba17ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample_img(img_loader, device, color=True):\n",
    "    _, (example_datas, labels) = next(enumerate(img_loader))\n",
    "    if(color):\n",
    "        sample = example_datas[0]\n",
    "        sample = sample.to(device)[None, :]\n",
    "    else:\n",
    "        sample = example_datas[0]\n",
    "        sample = torchvision.transforms.Grayscale()(sample)[0]\n",
    "        sample = sample.to(device)[None, None, :]\n",
    "    return sample\n",
    "\n",
    "\n",
    "# Setup tensorboard\n",
    "tb_parent_dir = './runs/'\n",
    "repo = git.Repo(search_parent_directories=True)\n",
    "sha = repo.head.object.hexsha\n",
    "#head = repo.head\n",
    "local_branch = repo.active_branch.name\n",
    "run_dir = local_branch + '/' + sha[-3:] + '/' +  datetime.datetime.now().isoformat(timespec='seconds') + '/'\n",
    "print('TB Log Directory is: ', tb_parent_dir + run_dir)\n",
    "writer = SummaryWriter(log_dir=tb_parent_dir + run_dir)\n",
    "\n",
    "# Setup model saving\n",
    "model_parent_dir = './model_checkpoints/'\n",
    "model_checkpoint_dir = model_parent_dir + local_branch + '/'\n",
    "path = Path(model_checkpoint_dir)\n",
    "path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "image_transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.RandomVerticalFlip(p=0.5),\n",
    "    torchvision.transforms.RandomHorizontalFlip(p=0.5),\n",
    "    #torchvision.transforms.RandomRotation(degrees=[0, 360], expand=True),\n",
    "    torchvision.transforms.ColorJitter(brightness=0.5, hue=0.3),\n",
    "    torchvision.transforms.RandomInvert(p=0.5),\n",
    "    torchvision.transforms.Resize((screen_h, screen_w))])\n",
    "\n",
    "train_dataset = torchvision.datasets.Flowers102('flowers102/', \n",
    "                                          split='train',\n",
    "                                          download=True,\n",
    "                                          transform=image_transform)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                           batch_size=1, \n",
    "                                           shuffle=True)\n",
    "\n",
    "\n",
    "grayscale = False\n",
    "print('Grayscale: ', grayscale)\n",
    "sample = get_sample_img(train_loader, device, color=not grayscale)\n",
    "print('Image shape: ', sample.shape)\n",
    "ih, iw = tuple(sample.shape[2:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca6f9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.autograd.set_detect_anomaly(True):\n",
    "    mse = torch.nn.MSELoss(reduce=False)\n",
    "    loss_fn = torch.nn.MSELoss()\n",
    "    model = AutoEncoder(num_scales=len(coupling_convs.keys())).cuda()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "    start_step = 0\n",
    "    max_steps = 100000\n",
    "    for train_step in range(start_step + 1, start_step + max_steps):\n",
    "        # Generate a new image\n",
    "        img = get_sample_img(train_loader, device, color=not grayscale)\n",
    "\n",
    "        print('img: ', img.shape)\n",
    "        #break\n",
    "        # Reset optimizer\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Run model\n",
    "        phases, coupling_constants = model(img)\n",
    "        img_hat = run_sim(phases, coupling_constants, coupling_convs)\n",
    "        print('img_hat: ', img_hat.shape)\n",
    "        loss = loss_fn(img_hat, img)\n",
    "        # Backprop\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print('Step {0} with loss: {1}'.format(train_step, loss.detach().cpu()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420a341b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25fd8ab1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3870805",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33120545",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2207a92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff675c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea387e16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c993ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35dbc3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3a2427",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e72359",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a63e34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0628372f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa25823",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
