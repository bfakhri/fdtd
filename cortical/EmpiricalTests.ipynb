{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c28e203",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "import git\n",
    "import sys\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "sys.path.append('/home/bij/Projects/fdtd/')\n",
    "import math\n",
    "import time\n",
    "import fdtd\n",
    "import fdtd.backend as bd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision.utils import save_image\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from PIL import Image\n",
    "import scipy\n",
    "from autoencoder import AutoEncoder\n",
    "import argparse\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import util\n",
    "from matplotlib.backends.backend_agg import FigureCanvasAgg\n",
    "from matplotlib.figure import Figure\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Process args.')\n",
    "parser.add_argument('-f', '--load-file', type=str, default=None,\n",
    "                    help='File to load params from before training starts. Overrides --load-step.')\n",
    "parser.add_argument('-l', '--load-step', type=str, default='0',\n",
    "                    help='Where to start training. If latest, will start at the latest checkpoint.')\n",
    "parser.add_argument('-s', '--save-steps', type=int, default='1000',\n",
    "                    help='How often to save the model.')\n",
    "parser.add_argument('-c', '--coverage-ratio', type=float, default=1.0,\n",
    "                    help='How much distance a wave can cover as a proportion of the diagonal length of the sim.')\n",
    "parser.add_argument('-m', '--max-steps', type=int, default='1000000000000000',\n",
    "                    help='How many steps to train.')\n",
    "parser.add_argument('-d', '--dry-run', type=bool, default=False,\n",
    "                    help='If true, does not save model checkpoint.')\n",
    "parser.add_argument('-rog', '--reset-grid-optim', default=False, action='store_true',\n",
    "                    help='If true, loads completely new params for the grid and optimizer.')\n",
    "parser.add_argument('-is', '--image-size', type=int, default=40,\n",
    "                    help='Size of each side of the image. Determines grid size.')\n",
    "parser.add_argument('-sc', '--image-scaler', type=int, default=1,\n",
    "                    help='How much to scale the entire simulation by (changes the dimensions of the model).')\n",
    "parser.add_argument('-oc', '--old-scaler', type=int, default=1,\n",
    "                    help='If the loaded file was scaled, that scaler value.')\n",
    "parser.add_argument('-bem', '--bypass-em', default=False, action='store_true',\n",
    "                    help='If set, will disable the EM component of the model.')\n",
    "parser.add_argument('-gray', '--grayscale', default=False, action='store_true',\n",
    "                    help='If set, will force the input and output images to be grayscale.')\n",
    "parser.add_argument('-thw', '--target-half-way', default=False, action='store_true',\n",
    "                    help='If set, will only produce loss for the timestep halfway up the coverage ratio.')\n",
    "parser.add_argument('-sds', '--source-down-scaler', type=int, default=1, \n",
    "                    help='How much to stride sources in the cortical substrate.')\n",
    "parser.add_argument('-sgd', '--use-sgd', default=False, action='store_true',\n",
    "                    help='If set, will switch to SGD instead of Adam. Useful for finetuning.')\n",
    "parser.add_argument('-ti', '--target-img', default=None, action='store_true',\n",
    "                    help='If set, will only process this image file.')\n",
    "args = parser.parse_args()\n",
    "\n",
    "args.load_file = 'model_checkpoints/stability-improvements/md_000000037800.pt'\n",
    "args.grayscale = True\n",
    "args.image_scaler = 4\n",
    "\n",
    "def get_sorted_paths(directory_list, target_ext='.png'):\n",
    "    path_list = []\n",
    "    for directory in directory_list:\n",
    "        paths = [join(directory,f) for f in listdir(directory) if isfile(join(directory, f)) and f.endswith(target_ext)]\n",
    "        print(f'Found {len(paths)} files in {directory}')\n",
    "        path_list += paths\n",
    "    path_list.sort()\n",
    "    return path_list\n",
    "\n",
    "def scale_img(img, s=2):\n",
    "    return torchvision.transforms.functional.resize(img, size=(img.shape[-2] * s, img.shape[-1] * s), interpolation=torchvision.transforms.InterpolationMode.NEAREST)\n",
    "\n",
    "#img_paths = get_sorted_paths(['./optical_illusions/'])\n",
    "img_paths = ['./optical_illusions/Cornsweet_illusion.png',\n",
    "     './optical_illusions/Ehrenstein_Illusion.png',\n",
    "     './optical_illusions/dots_Brightness-Optical-Illusion.png',\n",
    "     './optical_illusions/illusory_brigntness.png',\n",
    "     './optical_illusions/illusory_brigntness_reverse.png',\n",
    "     './optical_illusions/rectangle_Brightness-Optical-Illusion2.png']\n",
    "\n",
    "# ## Set Backend\n",
    "backend_name = \"torch\"\n",
    "fdtd.set_backend(backend_name)\n",
    "if(backend_name.startswith(\"torch.cuda\")):\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a2cd5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b571da65",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Resize((args.image_size*args.image_scaler, args.image_size*args.image_scaler))])\n",
    "\n",
    "img_file = img_paths[0]\n",
    "print(img_file.split('/')[-1].split('.')[0])\n",
    "img = Image.open(img_file)\n",
    "img = image_transform(img)[None, ...]\n",
    "if(args.grayscale):\n",
    "    img = torchvision.transforms.Grayscale()(img)[None, 0, ...]\n",
    "else:\n",
    "    img = torchvision.transforms.Grayscale()(img)\n",
    "print('Image shape: ', img.shape)\n",
    "ih, iw = tuple(img.shape[2:4])\n",
    "print('ih, iw: ', ih, iw)\n",
    "\n",
    "# Physics constants\n",
    "WAVELENGTH = 1550e-9 # meters\n",
    "SPEED_LIGHT: float = 299_792_458.0  # [m/s] speed of light\n",
    "GRID_SPACING = 0.1 * WAVELENGTH # meters\n",
    "\n",
    "\n",
    "\n",
    "# Size of grid boundary layer\n",
    "bw = 10*args.image_scaler\n",
    "# Create FDTD Grid\n",
    "grid_h, grid_w = (ih+bw*2, iw+bw*2)\n",
    "print('Grid height and width: ', grid_h, grid_w)\n",
    "# Boundaries with width bw\n",
    "grid = fdtd.Grid(\n",
    "    (grid_h, grid_w, 1),\n",
    "    grid_spacing=GRID_SPACING,\n",
    "    permittivity=1.0,\n",
    "    permeability=1.0,\n",
    ")\n",
    "\n",
    "# Calculate how long it takes a wave to cross the entire grid.\n",
    "grid_diag_cells = math.sqrt(grid_h**2 + grid_w**2)\n",
    "grid_diag_len = grid_diag_cells * GRID_SPACING\n",
    "grid_diag_steps = int(grid_diag_len/SPEED_LIGHT/grid.time_step)+1\n",
    "print('Time steps to cover entire grid: ', grid_diag_steps)\n",
    "# The number of steps is based on the coverage ratio.\n",
    "if(args.target_half_way):\n",
    "    em_steps = int(grid_diag_steps*args.coverage_ratio/2)\n",
    "else:\n",
    "    em_steps = int(grid_diag_steps*args.coverage_ratio)\n",
    "print('Time steps the grid will run for: ', em_steps)\n",
    "\n",
    "\n",
    "# Create learnable objects at the boundaries\n",
    "grid[  0: bw, :, :] = fdtd.LearnableAnisotropicObject(permittivity=2.5, name=\"xlow\", device=device)\n",
    "grid[-bw:   , :, :] = fdtd.LearnableAnisotropicObject(permittivity=2.5, name=\"xhigh\", device=device)\n",
    "grid[:,   0:bw, :] = fdtd.LearnableAnisotropicObject(permittivity=2.5, name=\"ylow\", device=device)\n",
    "grid[:, -bw:  , :] = fdtd.LearnableAnisotropicObject(permittivity=2.5, name=\"yhigh\", device=device)\n",
    "grid[:, :, 0] = fdtd.PeriodicBoundary(name=\"zbounds\")\n",
    "\n",
    "# Creat the cortical column sources\n",
    "grid[bw:bw+ih,bw:bw+iw,0] = fdtd.CorticalColumnPlaneSource(\n",
    "    period = WAVELENGTH / SPEED_LIGHT,\n",
    "    polarization = 'x', # BS value, polarization is not used.\n",
    "    name = 'cc',\n",
    "    source_stride = args.source_down_scaler,\n",
    ")\n",
    "\n",
    "# Object defining the cortical column substrate \n",
    "grid[bw:-bw, bw:-bw, :] = fdtd.LearnableAnisotropicObject(permittivity=2.5, is_substrate=True, name=\"cc_substrate\", device=device)\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "# The weights for the reconstruction loss at each em time step. \n",
    "loss_step_weights = torch.ones(em_steps, device=device)/em_steps\n",
    "#loss_step_weights = torch.nn.Parameter(torch.reshape(loss_step_weights, (-1, 1, 1, 1, 1)))\n",
    "loss_step_weights.requires_grad = True\n",
    "softmax = torch.nn.Softmax(dim=0)\n",
    "\n",
    "# Initialize the model and grid with default params.\n",
    "if(args.grayscale):\n",
    "    chans = 1\n",
    "else:\n",
    "    chans = 3\n",
    "model = AutoEncoder(num_em_steps=em_steps, grid=grid, input_chans=chans, output_chans=chans, source_stride=args.source_down_scaler, bypass_em=args.bypass_em).to(device)\n",
    "print('All grid objects: ', [obj.name for obj in grid.objects])\n",
    "grid_params_to_learn = []\n",
    "grid_params_to_learn += [util.get_object_by_name(grid, 'xlow').inverse_permittivity]\n",
    "grid_params_to_learn += [util.get_object_by_name(grid, 'xhigh').inverse_permittivity]\n",
    "grid_params_to_learn += [util.get_object_by_name(grid, 'ylow').inverse_permittivity]\n",
    "grid_params_to_learn += [util.get_object_by_name(grid, 'yhigh').inverse_permittivity]\n",
    "grid_params_to_learn += [util.get_object_by_name(grid, 'cc_substrate').inverse_permittivity]\n",
    "# Nonlinearity weights for the substrate. \n",
    "grid_params_to_learn += [util.get_object_by_name(grid, 'cc_substrate').nonlin_conv.weight]\n",
    "grid_params_to_learn += [util.get_object_by_name(grid, 'cc_substrate').nonlin_conv.bias]\n",
    "# Nonlinearity weights for the cortical columns. \n",
    "grid_params_to_learn += [util.get_source_by_name(grid, 'cc').nonlin_conv.weight]\n",
    "grid_params_to_learn += [util.get_source_by_name(grid, 'cc').nonlin_conv.bias]\n",
    "# The weights for the loss.\n",
    "grid_params_to_learn += [loss_step_weights]\n",
    "\n",
    "# Load saved params for model and optimizer.\n",
    "#checkpoint_steps = [int(cf.split('_')[-1].split('.')[0]) for cf in checkpoints]\n",
    "if(args.load_file is not None):\n",
    "    start_step = int(args.load_file.split('/')[-1].split('_')[-1].split('.')[0])\n",
    "    print('Loading model {0}. Starting at step {1}.'.format(args.load_file, start_step))\n",
    "    optimizer_path = args.load_file.rsplit('.', 1)[0] + '.opt'\n",
    "    grid_path = args.load_file.rsplit('.', 1)[0] + '.grd'\n",
    "    model.load_state_dict(torch.load(args.load_file))\n",
    "else:\n",
    "    if(args.load_step == 'latest'):\n",
    "        if(len(checkpoint_steps) > 0):\n",
    "            latest_idx = np.argmax(checkpoint_steps)\n",
    "            start_step = checkpoint_steps[latest_idx]\n",
    "            model_dict_path = model_checkpoint_dir + checkpoints[latest_idx]\n",
    "            optimizer_path = model_dict_path.rsplit('.', 1)[0] + '.opt'\n",
    "            grid_path = model_dict_path.rsplit('.', 1)[0] + '.grd'\n",
    "            print('Loading model {0} with optimizer {1} and grid {2}.'.format(model_dict_path, optimizer_path, grid_path))\n",
    "            model.load_state_dict(torch.load(model_dict_path))\n",
    "        else:\n",
    "            start_step = 0\n",
    "    elif(int(args.load_step) != 0):\n",
    "        if(int(args.load_step) not in checkpoint_steps):\n",
    "            print('Checkpoint {0} not found in {1}'.format(args.load_step, model_checkpoint_dir))\n",
    "            sys.exit()\n",
    "        start_step = int(args.load_step)\n",
    "        model_idx = np.where(np.array(checkpoint_steps) == start_step)[0][0]\n",
    "        model_dict_path = model_checkpoint_dir + checkpoints[model_idx]\n",
    "        optimizer_path = model_dict_path.rsplit('.', 1)[0] + '.opt'\n",
    "        grid_path = model_dict_path.rsplit('.', 1)[0] + '.grd'\n",
    "        print('Loading model {0} with optimizer {1} and grid {2}.'.format(model_dict_path, optimizer_path, grid_path))\n",
    "        model.load_state_dict(torch.load(model_dict_path))\n",
    "    else:\n",
    "        print('Starting model at step 0')\n",
    "        start_step = 0\n",
    "        optimizer_path = None\n",
    "        grid_path = None\n",
    "\n",
    "reset_optimizer = False\n",
    "if((grid_path is not None) and (not args.reset_grid_optim)):\n",
    "    print('Loading grid params...')\n",
    "    with torch.no_grad():\n",
    "        load_grid_params_to_learn = torch.load(grid_path)\n",
    "        for idx, tensor in enumerate(load_grid_params_to_learn):\n",
    "            if(tensor.shape == grid_params_to_learn[idx][...].shape):\n",
    "                grid_params_to_learn[idx][...] = tensor[...]\n",
    "            else:\n",
    "                if(idx == len(load_grid_params_to_learn) - 1):\n",
    "                    tensor = torch.squeeze(tensor)\n",
    "                # Interpolate the thing....\n",
    "                print('INFO: Shapes are mismatched: {0} vs {1}'.format(tensor[...].shape, grid_params_to_learn[idx][...].shape))\n",
    "                \n",
    "                # If this is a grid param, expand it over the spatial dims.\n",
    "                if(len(tensor.shape) > 1):\n",
    "                    reps = np.ones(len(tensor.shape), dtype=int)\n",
    "                    for i in range(len(reps)):\n",
    "                        reps[i] = int(args.image_scaler / args.old_scaler)\n",
    "                        if(i >= 1):\n",
    "                            break\n",
    "\n",
    "                    tensor_np_interp = scipy.ndimage.zoom(tensor.detach().numpy(), reps, order=1)\n",
    "                    grid_params_to_learn[idx][...] = torch.from_numpy(tensor_np_interp)\n",
    "                    print('INFO: Grid object scaled to shape: ', grid_params_to_learn[idx][...].shape)\n",
    "                # If this is the loss step weights, scale it linearly to fit the new size.\n",
    "                else:\n",
    "                    tensor_interp = torch.nn.functional.interpolate(tensor[None, None, ...], grid_params_to_learn[idx][...].shape, mode='linear')\n",
    "                    grid_params_to_learn[idx][...] = tensor_interp\n",
    "                    print('INFO: EM Step loss object scaled to shape: ', grid_params_to_learn[idx][...].shape)\n",
    "                    print('EM Steps: ', em_steps)\n",
    "                    print('Loss step weights: ', loss_step_weights.shape)\n",
    "\n",
    "                # Since parameter shapes have changed, the optimizer weights are obsolete.\n",
    "                reset_optimizer = True\n",
    "\n",
    "# Combine grid and model params and register them with the optimizer.\n",
    "params_to_learn = [*model.parameters()] + grid_params_to_learn\n",
    "\n",
    "mse = torch.nn.MSELoss(reduce=False)\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "grid.H.requires_grad = True\n",
    "grid.H.retain_grad()\n",
    "grid.E.requires_grad = True\n",
    "grid.E.retain_grad()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b122a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#old_weight = model.conv_linear.weight\n",
    "#model.conv_linear.weight = torch.nn.Parameter(torch.roll(model.conv_linear.weight, shifts=-1))\n",
    "#model.conv_linear.weight = torch.nn.Parameter(torch.roll(old_weight, shifts=3))\n",
    "#model.conv_linear.weight = torch.nn.Parameter(torch.Tensor(np.array([0,0,0,0,1,0]))[None, :, None, None])\n",
    "#model.conv_linear.weight = old_weight\n",
    "#model.conv_linear.weight\n",
    "#old_weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56fd368",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get sample from training data\n",
    "em_step_loss_weight_dist = softmax(loss_step_weights)\n",
    "argmax_step = torch.argmax(torch.squeeze(loss_step_weights))\n",
    "\n",
    "img_file = img_paths[0]\n",
    "# Reset the grid\n",
    "grid.reset()\n",
    "# Load an image\n",
    "img_name = img_file.split('/')[-1].split('.')[0]\n",
    "print('Processing: ', img_name)\n",
    "img = Image.open(img_file)\n",
    "img = image_transform(img)[None, ...]\n",
    "if(args.grayscale):\n",
    "    print('Making grayscale: ', img.shape)\n",
    "    img = torchvision.transforms.Grayscale()(img)[None, 0, ...]\n",
    "    print(img.shape)\n",
    "    #break\n",
    "else:\n",
    "    print('NOT making grayscale: ', img.shape)\n",
    "    img = torchvision.transforms.Grayscale()(img)\n",
    "    print(img.shape)\n",
    "\n",
    "for em_step, (img_hat_em, em_field) in enumerate(model(img, summary_writer=None, train_step=0)):\n",
    "    print(img_hat_em.shape, em_field.shape)\n",
    "    break\n",
    "    \n",
    "\n",
    "for em_step, (img_hat_em, em_field) in enumerate(model(img, summary_writer=None, train_step=0, output_scaler=5)):\n",
    "    print(img_hat_em.shape, em_field.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e602c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Get sample from training data\n",
    "em_step_loss_weight_dist = softmax(loss_step_weights)\n",
    "argmax_step = torch.argmax(torch.squeeze(loss_step_weights))\n",
    "\n",
    "# Only process one image if this is set\n",
    "if(args.target_img):\n",
    "    img_paths = [args.target_img]\n",
    "\n",
    "for img_file in img_paths:\n",
    "    # Reset the grid\n",
    "    grid.reset()\n",
    "    # Load an image\n",
    "    img_name = img_file.split('/')[-1].split('.')[0]\n",
    "    print('Processing: ', img_name)\n",
    "    img = Image.open(img_file)\n",
    "    img = image_transform(img)[None, ...]\n",
    "    if(args.grayscale):\n",
    "        img = torchvision.transforms.Grayscale()(img)[None, 0, ...]\n",
    "    else:\n",
    "        img = torchvision.transforms.Grayscale()(img)\n",
    "    \n",
    "    # Get the path of the test strip\n",
    "    illusion_path, illusion_filename = img_file.rsplit('/', 1)\n",
    "    test_strip_pref = illusion_path + '/test_strips/' + illusion_filename.split('.')[0]\n",
    "    test_strip_file_a = test_strip_pref + '_teststrip_part_a.png'\n",
    "    test_strip_file_b = test_strip_pref + '_teststrip_part_b.png'\n",
    "    print('Opening image {0} with test strip file {1}'.format(img_file, test_strip_file_a))\n",
    "    \n",
    "    try:\n",
    "        test_strip_img_a = np.array(Image.open(test_strip_file_a))\n",
    "    except:\n",
    "        print('Could not find {0}, skipping'.format(test_strip_file_a))\n",
    "        continue\n",
    "    try:\n",
    "        test_strip_img_b = np.array(Image.open(test_strip_file_b))\n",
    "    except:\n",
    "        print('Could not find {0}, making zeros.'.format(test_strip_file_b))\n",
    "        test_strip_img_b = np.zeros_like(test_strip_img_a)\n",
    "    # Make sure it is binary.\n",
    "    assert np.sum(((test_strip_img_a > 0) * (test_strip_img_a < 1)).astype(np.int)) <= 0\n",
    "    assert np.sum(((test_strip_img_b > 0) * (test_strip_img_b < 1)).astype(np.int)) <= 0\n",
    "    test_strip_img_a = image_transform(test_strip_img_a)[None, ...]\n",
    "    test_strip_img_b = image_transform(test_strip_img_b)[None, ...]\n",
    "    # Re-binarize after the resize.\n",
    "    test_strip_img_a = (test_strip_img_a > 0.5).float()\n",
    "    test_strip_img_b = (test_strip_img_b > 0.5).float()\n",
    "    print('Test strip stats: ', torch.mean(test_strip_img_a), torch.min(test_strip_img_a), torch.max(test_strip_img_a))\n",
    "    print('Test strip stats: ', torch.mean(test_strip_img_b), torch.min(test_strip_img_b), torch.max(test_strip_img_b))\n",
    "\n",
    "    for em_step, (img_hat_em, em_field) in enumerate(model(img, summary_writer=None, train_step=0)):\n",
    "        print(img_name, '\\tem step: ', em_step, em_steps, end='\\r')\n",
    "        \n",
    "        # Save the em field and energy at the target step.\n",
    "        e_field_img = em_field[0:3,...]\n",
    "        h_field_img = em_field[3:6,...]\n",
    "        \n",
    "        #if(em_step == argmax_step):\n",
    "        #if(em_step % 10 == 0):\n",
    "        if(True):\n",
    "            masked_output_a = test_strip_img_a * torch.unsqueeze(img_hat_em, 0)\n",
    "            masked_output_b = test_strip_img_b * torch.unsqueeze(img_hat_em, 0)\n",
    "            masked_output_a = torch.squeeze(masked_output_a)\n",
    "            masked_output_b = torch.squeeze(masked_output_b)\n",
    "            masked_output_a = torch.sum(masked_output_a, axis=-2)\n",
    "            masked_output_b = torch.sum(masked_output_b, axis=-2)\n",
    "            # Sum the input image over the y dimension.\n",
    "            print('img: ', img.shape)\n",
    "            input_img_masked_sum = test_strip_img_a * img\n",
    "            input_img_masked_sum = torch.squeeze(input_img_masked_sum)\n",
    "            input_img_masked_sum = torch.sum(input_img_masked_sum, axis=-2)\n",
    "            \n",
    "        fig = Figure(figsize=(4.8, 4.8), dpi=100)\n",
    "        fig.tight_layout(pad=0)\n",
    "        canvas = FigureCanvasAgg(fig)\n",
    "        # Do some plotting here\n",
    "        ax = fig.add_subplot(111)\n",
    "        ax.plot(masked_output_a.detach().numpy())\n",
    "        ax.plot(input_img_masked_sum.detach().numpy())\n",
    "        #ax.plot([1, 2, 3])\n",
    "        # Retrieve a view on the renderer buffer\n",
    "        canvas.draw()\n",
    "        buf = canvas.buffer_rgba()\n",
    "        # convert to a NumPy array\n",
    "        plot_img = torch.from_numpy(np.transpose(np.asarray(buf).astype(np.float32), (2,0,1))[0:3, ...])/255.0\n",
    "        print(plot_img.min(), plot_img.max())\n",
    "\n",
    "        print(input_img_masked_sum.shape)\n",
    "            #plt.show()\n",
    "        # Add the argmaxxed images to tensorboard\n",
    "        if(args.grayscale):\n",
    "            img_save =  scale_img(img.expand(-1, 3, -1, -1), s=3)\n",
    "            #img_hat_em_save =  img_hat_em_save.expand(3, -1, -1)\n",
    "            img_hat_em_save =  scale_img(img_hat_em, s=3).expand(3, -1, -1)\n",
    "        img_grid = torchvision.utils.make_grid([img_save[0,...], img_hat_em_save, plot_img,\n",
    "            util.norm_img_by_chan(scale_img(e_field_img, s=3)), \n",
    "            util.norm_img_by_chan(scale_img(h_field_img, s=3))])\n",
    "        save_image(img_grid, './images/{0}_step_{1}.png'.format(img_name, str(em_step).zfill(12)))\n",
    "    print('')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15bd4e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
